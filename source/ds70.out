Running model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]Loading checkpoint shards:   6%|▌         | 1/17 [00:01<00:22,  1.38s/it]Loading checkpoint shards:  12%|█▏        | 2/17 [00:02<00:17,  1.18s/it]Loading checkpoint shards:  18%|█▊        | 3/17 [00:02<00:10,  1.38it/s]Loading checkpoint shards:  24%|██▎       | 4/17 [00:03<00:10,  1.19it/s]Loading checkpoint shards:  29%|██▉       | 5/17 [00:04<00:10,  1.13it/s]Loading checkpoint shards:  35%|███▌      | 6/17 [00:05<00:10,  1.08it/s]Loading checkpoint shards:  41%|████      | 7/17 [00:06<00:09,  1.07it/s]Loading checkpoint shards:  47%|████▋     | 8/17 [00:07<00:08,  1.05it/s]Loading checkpoint shards:  53%|█████▎    | 9/17 [00:08<00:07,  1.04it/s]Loading checkpoint shards:  59%|█████▉    | 10/17 [00:09<00:06,  1.01it/s]Loading checkpoint shards:  65%|██████▍   | 11/17 [00:10<00:05,  1.01it/s]Loading checkpoint shards:  71%|███████   | 12/17 [00:11<00:04,  1.01it/s]Loading checkpoint shards:  76%|███████▋  | 13/17 [00:12<00:03,  1.01it/s]Loading checkpoint shards:  82%|████████▏ | 14/17 [00:13<00:03,  1.00s/it]Loading checkpoint shards:  88%|████████▊ | 15/17 [00:14<00:01,  1.00it/s]Loading checkpoint shards:  94%|█████████▍| 16/17 [00:15<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 17/17 [00:16<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 17/17 [00:16<00:00,  1.01it/s]
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Token indices sequence length is longer than the specified maximum sequence length for this model (16842 > 16384). Running this sequence through the model will result in indexing errors
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running task: trip
Running example: trip_planning_example_1088
Running example: trip_planning_example_1424
Running example: trip_planning_example_344
Running example: trip_planning_example_1392
Running example: trip_planning_example_500
Running example: trip_planning_example_1097
Running example: trip_planning_example_421
Running example: trip_planning_example_1075
Running example: trip_planning_example_1370
Running example: trip_planning_example_1116
Running example: trip_planning_example_762
Running example: trip_planning_example_1511
Running example: trip_planning_example_587
Running example: trip_planning_example_90
Running example: trip_planning_example_1487
Running example: trip_planning_example_684
Running example: trip_planning_example_1434
Running example: trip_planning_example_1568
Running example: trip_planning_example_464
Running example: trip_planning_example_1572
Running example: trip_planning_example_126
Running example: trip_planning_example_824
Running example: trip_planning_example_1502
Running example: trip_planning_example_1094
Running example: trip_planning_example_995
Running example: trip_planning_example_810
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/hz466/calendar-planning/source/generate_smt_input.py", line 100, in <module>
    # Save raw response text
OSError: [Errno 28] No space left on device
