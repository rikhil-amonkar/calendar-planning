Running model: meta-llama/Llama-3.1-70B-Instruct
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:17,  1.68it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:01<00:16,  1.65it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:01<00:16,  1.60it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:02<00:15,  1.64it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:03<00:14,  1.69it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:03<00:14,  1.68it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:04<00:13,  1.66it/s]Loading checkpoint shards:  27%|██▋       | 8/30 [00:04<00:13,  1.62it/s]Loading checkpoint shards:  30%|███       | 9/30 [00:05<00:12,  1.63it/s]Loading checkpoint shards:  33%|███▎      | 10/30 [00:06<00:12,  1.66it/s]Loading checkpoint shards:  37%|███▋      | 11/30 [00:06<00:11,  1.67it/s]Loading checkpoint shards:  40%|████      | 12/30 [00:07<00:10,  1.71it/s]Loading checkpoint shards:  43%|████▎     | 13/30 [00:07<00:10,  1.65it/s]Loading checkpoint shards:  47%|████▋     | 14/30 [00:08<00:09,  1.62it/s]Loading checkpoint shards:  50%|█████     | 15/30 [00:09<00:09,  1.63it/s]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:09<00:08,  1.68it/s]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:10<00:07,  1.72it/s]Loading checkpoint shards:  60%|██████    | 18/30 [00:10<00:07,  1.71it/s]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:11<00:06,  1.71it/s]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:11<00:05,  1.74it/s]Loading checkpoint shards:  70%|███████   | 21/30 [00:12<00:05,  1.76it/s]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:13<00:04,  1.78it/s]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:13<00:03,  1.75it/s]Loading checkpoint shards:  80%|████████  | 24/30 [00:14<00:03,  1.74it/s]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:14<00:02,  1.76it/s]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:15<00:02,  1.77it/s]Loading checkpoint shards:  90%|█████████ | 27/30 [00:15<00:01,  1.78it/s]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:16<00:01,  1.75it/s]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:17<00:00,  1.74it/s]Loading checkpoint shards: 100%|██████████| 30/30 [00:17<00:00,  2.09it/s]Loading checkpoint shards: 100%|██████████| 30/30 [00:17<00:00,  1.73it/s]
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Running task: trip
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1088.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1424.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_344.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1392.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_500.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1097.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_421.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1075.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1370.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1116.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_762.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1511.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_587.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_90.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1487.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_684.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1434.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1568.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_464.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1572.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_126.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_824.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1502.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1094.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_995.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_810.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1167.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_857.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_361.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1534.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_517.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_664.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_50.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1559.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_372.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_993.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_564.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1480.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_371.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_875.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_586.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_505.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_934.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1009.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1596.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_87.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_580.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_59.py, file already exists.
Skipping ../output/SMT/Llama-3.1-70B-Instruct/trip/code/trip_planning_example_1324.py, file already exists.
Running example: trip_planning_example_1450
Running example: trip_planning_example_1060
Running example: trip_planning_example_21
Running example: trip_planning_example_339
Running example: trip_planning_example_323
Running example: trip_planning_example_81
Running example: trip_planning_example_996
Running example: trip_planning_example_591
Running example: trip_planning_example_125
Running example: trip_planning_example_915
Running example: trip_planning_example_1066
Running example: trip_planning_example_92
Running example: trip_planning_example_29
Running example: trip_planning_example_288
Running example: trip_planning_example_1509
Running example: trip_planning_example_674
Running example: trip_planning_example_116
Running example: trip_planning_example_675
Running example: trip_planning_example_1500
Running example: trip_planning_example_455
Running example: trip_planning_example_188
Running example: trip_planning_example_240
Running example: trip_planning_example_769
Running example: trip_planning_example_813
Running example: trip_planning_example_409
Running example: trip_planning_example_1543
Running example: trip_planning_example_1147
Running example: trip_planning_example_1164
Running example: trip_planning_example_253
Running example: trip_planning_example_1148
Running example: trip_planning_example_1161
Running example: trip_planning_example_149
Running example: trip_planning_example_768
Running example: trip_planning_example_1432
Running example: trip_planning_example_1318
Running example: trip_planning_example_656
Running example: trip_planning_example_950
Running example: trip_planning_example_275
Running example: trip_planning_example_777
Running example: trip_planning_example_113
Running example: trip_planning_example_440
Running example: trip_planning_example_953
Running example: trip_planning_example_699
Running example: trip_planning_example_1549
An error occurred: CUDA out of memory. Tried to allocate 2.98 GiB. GPU 0 has a total capacity of 93.10 GiB of which 2.62 GiB is free. Including non-PyTorch memory, this process has 90.47 GiB memory in use. Of the allocated memory 83.66 GiB is allocated by PyTorch, and 6.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables). Retrying in 5 seconds...
Running model: meta-llama/Llama-3.1-70B-Instruct
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:07,  4.00it/s]Loading checkpoint shards: 100%|██████████| 30/30 [00:00<00:00, 96.01it/s]
Some parameters are on the meta device because they were offloaded to the cpu.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
